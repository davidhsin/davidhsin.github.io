## HPC Camp è§£é¡Œ


# Mandelbrot Set

### TODO:
~~**1. Use MPI to parallelize across different processes.**~~

~~**2. Use OpenMP to collapse the two for loops.**~~

**3. Implement load balancing.**

### å•é¡Œ
æ ¹æ“šé¡Œç›®çµ¦çš„æç¤ºï¼Œç¬¬1é»å’Œç¬¬2é»å…¶å¯¦éƒ½ä¸æ˜¯å¤§å•é¡Œï¼Œæˆ‘èªç‚ºæ¯”è¼ƒé›£çš„é»æ˜¯ load balancingï¼Œæ‡‰è©²æ€éº¼åˆç†çš„åˆ†é…å·¥ä½œçµ¦ä¸åŒçš„ Processes ï¼Ÿ

èµ·åˆè¨­å®šäº†æ¯å€‹ process éƒ½åˆ†é…åˆ°ä¸€æ¨£çš„ rows æ•¸ï¼Œä½†å…¶å¯¦æ¯å€‹ row (æˆ–è€…èªª pixel)çš„å·¥ä½œé‡å·®ç•°ç”šå¤§ï¼Œé€™æ˜¯å°è‡´ TLE çš„ä¸»å› ã€‚
![Screenshot 2024-08-14 at 11.43.58](https://hackmd.io/_uploads/HJcW_iY90.png)

å·¥ä½œåˆ†é…ä¸å‡çš„çµæœï¼Œå°±ç®—ACä¹Ÿåƒ…æ˜¯å£“ç·šéï¼š
![image](https://hackmd.io/_uploads/HkkProF50.png)

ç¨å¾®è§€å¯Ÿæœ€å…§å±¤è¿´åœˆå¯ä»¥ç™¼ç¾ï¼Œè‹¥è¦åœ¨åœ–ç‰‡ä¸Šå¡—ä¸Šé»‘è‰² pixelï¼Œé‚£éº¼å¿…é ˆè·‘å®Œ10001æ¬¡è¿´åœˆï¼Œå…¶é¤˜é¡è‰²å‰‡ä¸ç”¨è·‘é‚£éº¼å¤šã€‚ä¹Ÿå°±æ˜¯èªªç¹é‡çš„å·¥ä½œéƒ½åœ¨é»‘è‰²éƒ¨åˆ†ã€‚

![Screenshot 2024-08-14 at 11.37.47](https://hackmd.io/_uploads/BJF9LsKcR.png)


### åˆ†é…å·¥ä½œé‡
#### 1. åˆ†ææ¯å€‹ row çš„é‡è¤‡æ¬¡æ•¸ã€‚
![image](https://hackmd.io/_uploads/SJszMjF9A.png)
é€™æ˜¯ç¬¬ä¸€ç­†æ¸¬è³‡ï¼ˆHEIGHT: 600 WIDTH: 800ï¼‰å…±600 rowsï¼Œå¯ä»¥ç™¼ç¾æ„ˆé è¿‘ä¸­é–“è¨ˆç®—é‡æ„ˆå¤§ï¼Œç”šè‡³è¶…éäº†ä¸‰ç™¾è¬ã€‚

![Screenshot 2024-08-14 at 17.20.29](https://hackmd.io/_uploads/BJlGePg5qR.png)
åœ¨row 301 è™•è¿´åœˆç”šè‡³è¦è·‘450è¬æ¬¡å»è™•ç†åƒ…800å€‹ pixelsã€‚

![Screenshot 2024-08-14 at 17.21.22](https://hackmd.io/_uploads/Bkf7Pg55R.png)
èˆ‡å‰å¹¾ row ç›¸æ¯”ï¼Œå¯è¦‹éå¸¸çš„ä¸å¹³è¡¡ã€‚


#### 2. æ†‘æ„Ÿè¦ºåˆ‡ :)
æƒ³æ³•éå¸¸çš„ç„¡è…¦ï¼Œå°‡ç¸½ rows æ•¸ä»¥æ¸¬è³‡è¦å®šçš„ process æ•¸å¹³åˆ†çµ¦æ¯å€‹ process ï¼Œå†å°‡ä¸­é–“éƒ¨åˆ†çš„å·¥ä½œé‡é™¤ä»¥4ï¼Œæ‰£é™¤çš„å·¥ä½œåˆ†çµ¦é ­å°¾å…©å€‹è² æ“”è¼ƒè¼•çš„ process ã€‚

ä»¥ç¬¬ä¸€ç­†æ¸¬è³‡ç‚ºä¾‹ï¼š
```
height=600
width=800

# Judge specs
processes=6
threads=4
timelimit=5
```

![Comp MPI 2](https://hackmd.io/_uploads/S17peCF90.jpg)

ä¸€å®šæœ‰æ›´å®Œç¾çš„åˆ‡æ³•ï¼Œä½†æˆ‘ä¸æœƒğŸ¤·â€â™‚ï¸ã€‚btw, Mandelbrot Set è£¡é¢æš—è—è‘— fibonacci sequence ï¼Œæˆ–è¨±ç…§è‘—1 1 2 3 5 8çš„æ¯”ä¾‹å»åˆ‡æœƒæœ‰æ›´å¥½çš„æ•ˆèƒ½ï¼Ÿ

### mandelbrot_parallel.cpp
```cpp=
#define STB_IMAGE_WRITE_IMPLEMENTATION
#include <string.h>
#include <algorithm>
#include <complex>
#include <iostream>
#include <vector>
#include <mpi.h>
#include <omp.h>
#include "stb_image_write.h"

#define MaxRepeats 10000

using namespace std;

int HEIGHT, WIDTH;

struct Color {
    unsigned char r, g, b;
};

complex<double> trap(0.5, 0);

void hsv_to_rgb(double h, double s, double v,
    unsigned char &r, unsigned char &g, unsigned char &b) {
    double c = v * s;
    double x = c * (1 - abs(fmod(h / 60.0, 2) - 1));
    double m = v - c;
    double r1, g1, b1;

    if (h >= 0 && h < 60) {
        r1 = c;
        g1 = x;
        b1 = 0;
    } else if (h >= 60 && h < 120) {
        r1 = x;
        g1 = c;
        b1 = 0;
    } else if (h >= 120 && h < 180) {
        r1 = 0;
        g1 = c;
        b1 = x;
    } else if (h >= 180 && h < 240) {
        r1 = 0;
        g1 = x;
        b1 = c;
    } else if (h >= 240 && h < 300) {
        r1 = x;
        g1 = 0;
        b1 = c;
    } else {
        r1 = c;
        g1 = 0;
        b1 = x;
    }

    r = static_cast<unsigned char>((r1 + m) * 255);
    g = static_cast<unsigned char>((g1 + m) * 255);
    b = static_cast<unsigned char>((b1 + m) * 255);
}

/*
TODO:
1. Use MPI to parallelize across different processes.
2. Use OpenMP to collapse the two for loops.
3. Implement load balancing.
*/

void distribute_work(int size, int rank, int& start_row, int& end_row) {
    const int rows_for_middle = std::ceil(static_cast<float>(HEIGHT) / size / 4); 
    const int rows_for_start = (HEIGHT - ((size - 2) * rows_for_middle)) / 2; 

    if (rank == 0) {
        start_row = 0;
        end_row = start_row + rows_for_start;
    } else if (rank == size - 1) {
        start_row = HEIGHT - rows_for_start;
        end_row = HEIGHT;
    } else {
        start_row = rows_for_start + (rank - 1) * rows_for_middle;
        end_row = start_row + rows_for_middle;
    }
}

int main(int argc, char *argv[]) {
    if (argc < 3) {
        std::cout << "Usage: " << argv[0] << " <inputfile> <outputfile>" << std::endl;
        return 1;
    }

    char *inputfile_name = argv[1];
    string outputfile_name = argv[2];

    if (freopen(inputfile_name, "r", stdin) == nullptr) {
        std::cout << "Error: Unable to open input file: " << inputfile_name << std::endl;
        return 1;
    }

    scanf("height=%d\n", &HEIGHT);
    scanf("width=%d", &WIDTH);
    cout << "Width: " << WIDTH << " Height: " << HEIGHT << endl;

    // åˆå§‹åŒ– MPI
    int size, rank; 
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int start_row, end_row;
    distribute_work(size, rank, start_row, end_row);
    std::vector<unsigned char> partial_image((end_row - start_row) * WIDTH * 3);

#pragma omp parallel for collapse(2) schedule(dynamic)
    for (int y = start_row; y < end_row; y++) {
        for (int x = 0; x < WIDTH; x++) { 
            complex<double> c((x - WIDTH / 2.0) * 4.0 / WIDTH, (y - HEIGHT / 2.0) * 4.0 / HEIGHT);
            complex<double> z = 0;
            double min_dist = numeric_limits<double>::max();
            int repeats = 0;
            for (repeats = 0; repeats <= MaxRepeats; repeats++) { // æœ€å¤šè·‘10001æ¬¡
                z = z * z + c;
                double dist = abs(z - trap);
                if (dist < min_dist) {
                    min_dist = dist;
                }
                // ä¸æ˜¯é»‘è‰²ï¼Œææ—©è·³å‡ºè¿´åœˆï¼Œè¨ˆç®—é‡å°ã€‚
                if (abs(z) > 2.0) break;
            }
            int index = ((y - start_row) * WIDTH + x) * 3;

            // é»‘è‰² pixel ï¼Œè¿´åœˆéƒ½è¦è·‘å®Œ10001æ¬¡ï¼Œè¨ˆç®—é‡è¶…å¤§ã€‚
            if (repeats > MaxRepeats) {
                partial_image[index] = 0;
                partial_image[index + 1] = 0;
                partial_image[index + 2] = 0;
            } else {
                // éé»‘è‰²ï¼Œè¿´åœˆè·‘çš„æ¬¡æ•¸å°æ–¼10001æ¬¡
                hsv_to_rgb(360.0 * min_dist / 2.0, 1.0, repeats < MaxRepeats ? 1.0 : 0.0, 
                    partial_image[index], partial_image[index + 1], partial_image[index + 2]);
            }
        }
    }

    // érank 0 process å°‡è‡ªå·±éƒ¨åˆ†çš„è¨ˆç®—çµæœå‚³çµ¦Main Processï¼Œ
    if (rank != 0) {
        MPI_Send(
            partial_image.data(),              // buf:      initial address of send buffer (choice)
            (end_row - start_row) * WIDTH * 3, // count:    number of elements in send buffer (nonnegative integer)
            MPI_UNSIGNED_CHAR,                 // datatype: datatype of each send buffer element (handle)
            0,                                 // dest:     rank of destination (integer)
            0,                                 // tag:      message tag (integer)
            MPI_COMM_WORLD);                   // comm:     communicator (handle)
    } else {
        // Main Process å°‡å¤šå€‹ process çš„çµæœé›†åˆèµ·ä¾†
        unsigned char *image = new unsigned char[HEIGHT * WIDTH * 3];
        std::copy(partial_image.begin(), partial_image.end(), image); // Copy own part

        for (int i = 1; i < size; i++) {
            int recv_start_row, recv_end_row;
            distribute_work(size, i, recv_start_row, recv_end_row);
            std::vector<unsigned char> recv_buffer((recv_end_row - recv_start_row) * WIDTH * 3);

            MPI_Recv(
                recv_buffer.data(),                          // buf:      initial address of receive buffer (choice) 
                (recv_end_row - recv_start_row) * WIDTH * 3, // count:    maximum number of elements in receive buffer (integer)
                MPI_UNSIGNED_CHAR,                           // datatype: datatype of each receive buffer element (handle)
                i,                                           // source:   rank of source (integer)
                0,                                           // tag:      message tag (integer)
                MPI_COMM_WORLD,                              // comm:     communicator (handle)
                MPI_STATUS_IGNORE);                          // status:   status object (Status)
            std::copy(recv_buffer.begin(), recv_buffer.end(), image + (recv_start_row * WIDTH * 3));
        }

        stbi_write_png(outputfile_name.c_str(), WIDTH, HEIGHT, 3, image, WIDTH * 3);
        cout << "Image saved to " << outputfile_name << endl;
        delete[] image;
    }

	MPI_Finalize();
    return 0;
}
```

### Final Resultï¼š
![IMG_1227 (1)](https://hackmd.io/_uploads/BymOfAY90.jpg)

### References:
[MPI_Send](https://www.mpich.org/static/docs/v3.3/www3/MPI_Send.html)

[MPI_Recv](https://www.mpich.org/static/docs/v3.2/www3/MPI_Recv.html)

[MPI_Send example code](https://rookiehpc.org/mpi/docs/mpi_send/index.html)
[ç¬¬10è¬› Embarrassingly Computations & Load Balancing](https://reurl.cc/vv0bgk)
[Parallel Fractal Image Generation](http://matthiasbook.de/papers/parallelfractals/mandelbrot.html)
