## This is my first blog post



```c
#include <stdio.h>
int main(int argc, char *argv[]) {
    pirntf("Hello there.\n");
    return 0;
}
 ```
### PPO
![PPO Algorithm Diagram](/public/IMG_0111.jpg)

*Figure 1: Overview of the PPO algorithm showing the rollout phase (data collection) and training phase (model updates).*

A typical PPO algorithm consists of an agent that interacts with an environment, collects data from the interaction, then updates its weights w.r.t an objective function. Specifically, 

1. The **Actor NN** outputs `num_actions` logits, from which we derive our policy. Based off of that policy, we take an action in the environment.
2. After interacting with the environment, we note the following and store it in memory: 

    a. **observation**: the current state or input the agent receives from the environment

    b. **action**: The action taken by the agent

    c. **log probabilities**: the log probability of the chosen action under the current policy

    d. **values**: the critic-predicted value of the current state, representing expected future rewards.

    e. **reward**: the scalar feedback signal received from the environment indicating how good the action was

    f. **terminated**: a boolean indicating whether the episode has ended
    
3. The experience data is used as calculation in the training phase, particularly to calculate the logits using the Actor NN and the values using the critic NN.

4. Three values are calculated, including the logits and values derived from the experience. 

    a. **entropy**: the amount of uncertainity a distribution has. Given a discrete probability distribution $p$, entropy is given by 

    $$H(p) = \sum_x p(x) \ln\left(\frac1{p(x)}\right)$$
